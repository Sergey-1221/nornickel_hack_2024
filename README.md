Мультимодальная обработка документов и извлечение эмбеддингов
Данный проект демонстрирует пример пайплайна для обработки PDF-документов, извлечения текстовых чанков и изображений из них, а также построения эмбеддингов для текста и изображений с помощью моделей Qwen2-VL. Полученные эмбеддинги могут быть затем использованы для мультимодального поиска и анализа.

Основные этапы
Этап 1: Конвертация PDF в чанки и изображения
Извлечение структуры PDF:
Используется docling для извлечения текста, структуры документа и изображений. В результате формирования JSON-файла, содержащего список чанков с текстом, метаданными (заголовки, номера страниц) и путями к извлечённым изображениям.

Чанкинг текста:
Применяется HierarchicalChunker для разбиения длинного текста на чанки определённого размера (chunk_size) с перекрытиями (chunk_overlap) и возможностью деления по заголовкам.

Сохранение результатов:
На выходе создаётся JSON-файл (например, documentname_processed.json) в каталоге processed_results_stage1/, где для каждого чанка хранится текст и список связанных изображений.

Этап 2: Построение эмбеддингов
Текстовые эмбеддинги:
Исполняется модель Qwen2-VL для получения текстовых эмбеддингов каждого чанка. Это позволяет в дальнейшем выполнять семантический поиск и сравнение текстовых фрагментов.

Эмбеддинги изображений:
Те же модели используются для извлечения эмбеддингов из изображений, найденных в документе. Изображения кодируются в векторное пространство, что позволяет осуществлять мультимодальный поиск — например, искать близкие по смыслу картинки к определённому тексту или наоборот.

Обновление JSON:
В результатах (processed_results_final/) создаётся обновлённый JSON, где у каждого чанка есть поля text_embedding и image_embeddings.

Структура репозитория
data/: исходные PDF-файлы для обработки.
processed_results_stage1/: результаты первого этапа — извлечённый текст, изображения и базовая структура чанков.
processed_results_final/: результаты второго этапа — JSON с добавленными текстовыми и визуальными эмбеддингами.
app.py или другой скрипт (не показан здесь, но подразумевается): может быть создан для интерактивного просмотра данных через Streamlit или для выполнения мультимодального поиска.
Требования к окружению
Python 3.8+

Установленные зависимости (укажите необходимые зависимости в requirements.txt или установите вручную):

bash
Копировать код
pip install docling transformers pillow torch torchvision torchaudio bitsandbytes
Примечание: Модель Qwen2-VL-7B-Instruct и docling требуют дополнительной настройки или наличия GPU для оптимального функционирования.

Запуск
Подготовка данных:
Поместите ваш PDF-файл в директорию data/.

Запуск этапа 1:

bash
Копировать код
python your_script.py
(где your_script.py — это скрипт, содержащий main_stage1(); например, код, предоставленный выше, вы можете сохранить как pipeline.py и затем выполнить python pipeline.py).

После выполнения в processed_results_stage1/ появится файл *_processed.json.

Запуск этапа 2 (опционально): Раскомментируйте вызов main_stage2() в коде или выполните отдельно при необходимости:

bash
Копировать код
# Внутри pipeline.py (после main_stage1) раскомментируйте main_stage2():
# main_stage2()
Затем снова:

bash
Копировать код
python pipeline.py
В processed_results_final/ появится обновлённый JSON с эмбеддингами.

Применение результатов
После получения эмбеддингов можно реализовать мультимодальный поиск: подавая на вход текстовый запрос, можно находить близкие чанки текста или связанные с ними изображения. Аналогично можно использовать изображение для поиска релевантных текстовых чанков и изображений.
Результаты могут быть интегрированы в Streamlit-приложение для интерактивного просмотра, фильтрации и анализа.
Дополнительная информация
Qwen2-VL: Модель для генерации текстовых и визуальных эмбеддингов. Подробности и документация — Qwen Models on HuggingFace.
docling: Инструмент для извлечения структурированных данных из документов. См. Docling GitHub.
Поддержка и развитие
Данный пример — только шаблон. Вы можете расширить функциональность, добавить собственные модели, улучшить качество чанкинга, интегрировать векторные базы данных для быстрого поиска или расширить инструменты визуализации результатов. Вопросы, предложения и улучшения приветствуются через Issues или Pull Requests в репозитории.

