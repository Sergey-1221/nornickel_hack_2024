{
 "cells": [
  {
   "cell_type": "code",
   "id": "ae4c1035-5204-4f22-90b0-8e2491e3c6cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T16:47:45.110472Z",
     "start_time": "2024-12-07T16:47:44.974965Z"
    }
   },
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode\n",
    "from docling_core.transforms.chunker import HierarchicalChunker\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "pipeline_options = PdfPipelineOptions(do_table_structure=True)\n",
    "pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE  # use more accurate TableFormer model\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'docling'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdocling\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatamodel\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase_models\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InputFormat\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdocling\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocument_converter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DocumentConverter, PdfFormatOption\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdocling\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatamodel\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipeline_options\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PdfPipelineOptions, TableFormerMode\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'docling'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad3577a-6cce-40eb-a098-6dcdc179c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from PIL import Image\n",
    "import torch\n",
    "from colpali_engine import ColPali\n",
    "from transformers.models.paligemma.configuration_paligemma import PaliGemmaConfig\n",
    "import gradio as gr\n",
    "\n",
    "def setup_models():\n",
    "    \n",
    "    try:\n",
    "        pali_config = PaliGemmaConfig(\n",
    "            vocab_size=32000,\n",
    "            hidden_size=4096,\n",
    "            intermediate_size=11008,\n",
    "            num_hidden_layers=32,\n",
    "            num_attention_heads=32,\n",
    "            max_position_embeddings=8192,\n",
    "            rms_norm_eps=1e-6,\n",
    "            use_cache=True,\n",
    "            pad_token_id=0,\n",
    "            bos_token_id=1,\n",
    "            eos_token_id=2,\n",
    "            tie_word_embeddings=False,\n",
    "            use_memory_efficient_attention=True,\n",
    "            hidden_act=\"silu\"\n",
    "        )\n",
    "        \n",
    "        # Инициализация ColPali\n",
    "        colpali = ColPali(config=pali_config)\n",
    "        \n",
    "        # Загружаем Qwen2-VL напрямую из transformers\n",
    "        from transformers import Qwen2VLForCausalLM\n",
    "        \n",
    "        qwen_model = Qwen2VLForCausalLM.from_pretrained(\n",
    "            \"Qwen/Qwen2-VL-7B\",\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        qwen_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"Qwen/Qwen2-VL-7B\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        return colpali, qwen_model, qwen_tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке моделей: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_image(image, colpali, qwen_model, qwen_tokenizer):\n",
    "    try:\n",
    "        # ColPali обработка\n",
    "        colpali_result = colpali.process_image(image)\n",
    "        \n",
    "        # Qwen2-VL обработка\n",
    "        qwen_inputs = qwen_tokenizer(\n",
    "            text=\"Describe this image in detail:\",\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(qwen_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            qwen_outputs = qwen_model.generate(\n",
    "                **qwen_inputs,\n",
    "                max_new_tokens=100,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        \n",
    "        qwen_result = qwen_tokenizer.decode(qwen_outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return f\"\"\"\n",
    "{colpali_result}\n",
    "\n",
    "{qwen_result}\n",
    "\"\"\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return (e)\n",
    "\n",
    "def main():\n",
    "    login_to_hf()\n",
    "    \n",
    "    colpali, qwen_model, qwen_tokenizer = setup_models()\n",
    "    \n",
    "    demo = gr.Interface(\n",
    "        fn=lambda img: process_image(img, colpali, qwen_model, qwen_tokenizer),\n",
    "        inputs=gr.Image(type=\"pil\"),\n",
    "        outputs=gr.Textbox(label=\"Результат\"),\n",
    "        title=\"ColPali + Qwen2-VL Demo\",\n",
    "        description=\"Загрузите изображение для анализа обеими моделями\"\n",
    "    )\n",
    "    \n",
    "    demo.launch(share=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c516ff-3810-49f5-9d18-92ee1a9a5bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloadeчd MALM to data/MALM.pdf\n",
      "Downloadeчd BILLY to data/BILLY.pdf\n",
      "Downloadeчd BOAXEL to data/BOAXEL.pdf\n",
      "Downloadeчd ADILS to data/ADILS.pdf\n",
      "Downloadeчd MICKE to data/MICKE.pdf\n",
      "Downloaded files: ['MICKE.pdf', 'BOAXEL.pdf', 'ADILS.pdf', 'MALM.pdf', 'BILLY.pdf']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "pdfs = {\n",
    "    \"MALM\": \"https://www.ikea.com/us/en/assembly_instructions/malm-4-drawer-chest-white__AA-2398381-2-100.pdf\",\n",
    "    \"BILLY\": \"https://www.ikea.com/us/en/assembly_instructions/billy-bookcase-white__AA-1844854-6-2.pdf\",\n",
    "    \"BOAXEL\": \"https://www.ikea.com/us/en/assembly_instructions/boaxel-wall-upright-white__AA-2341341-2-100.pdf\",\n",
    "    \"ADILS\": \"https://www.ikea.com/us/en/assembly_instructions/adils-leg-white__AA-844478-6-2.pdf\",\n",
    "    \"MICKE\": \"https://www.ikea.com/us/en/assembly_instructions/micke-desk-white__AA-476626-10-100.pdf\"\n",
    "}\n",
    "\n",
    "output_dir = \"data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for name, url in pdfs.items():\n",
    "    response = requests.get(url)\n",
    "    pdf_path = os.path.join(output_dir, f\"{name}.pdf\")\n",
    "\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f\"Downloadeчd {name} to {pdf_path}\")\n",
    "\n",
    "print(\"Downloaded files:\", os.listdir(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c72063a-b67b-44ce-a604-41f83289e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling_core.transforms.chunker import HierarchicalChunker\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e576277d-c49c-4109-adeb-01bab75446cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _convert_document(self, document_path: str):\n",
    "        print(f\"\\n=== Шаг 1: Конвертация документа ===\")\n",
    "        print(f\"Исходный файл: {document_path}\")\n",
    "        \n",
    "        result = self.converter.convert(document_path)\n",
    "        \n",
    "        print(f\"Тип документа: {type(result.document)}\")\n",
    "        print(f\"Количество страниц: {result.document.num_pages if hasattr(result.document, 'num_pages') else 'Неизвестно'}\")\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f712e246-b7bc-4f2c-8a7e-45f46b242ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _get_chunks(self, doc):        \n",
    "        chunks = list(self.chunker.chunk(doc))\n",
    "        \n",
    "        if chunks:\n",
    "            print(f\"Текст: {chunks[0].text[:200]}...\" if hasattr(chunks[0], 'text'))\n",
    "            print(f\"Заголовки: {chunks[0].meta.headings if hasattr(chunks[0].meta, 'headings') )\n",
    "        \n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a115304-85d1-44e5-82e0-5ec8200cdba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _process_content(self, chunks: List) -> List[Dict]:\n",
    "        \n",
    "        processed_chunks = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            chunk_data = {\n",
    "                'text': chunk.text if hasattr(chunk, 'text') else None,\n",
    "                'metadata': {\n",
    "                    'headings': chunk.meta.headings if hasattr(chunk.meta, 'headings') else [],\n",
    "                    'page_number': chunk.meta.doc_items[0].prov[0].page_no if hasattr(chunk.meta, 'doc_items') else None,\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            if hasattr(chunk.meta, 'doc_items') and chunk.meta.doc_items:\n",
    "                bbox = chunk.meta.doc_items[0].prov[0].bbox\n",
    "                chunk_data['metadata']['position'] = {\n",
    "                    'left': bbox.l,\n",
    "                    'top': bbox.t,\n",
    "                    'right': bbox.r,\n",
    "                    'bottom': bbox.b\n",
    "                }\n",
    "            \n",
    "            processed_chunks.append(chunk_data)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print({i}/{len(chunks)})\n",
    "        \n",
    "        print({len(processed_chunks)})\n",
    "        return processed_chunks\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1abe5bc-036b-45b7-aa71-6f784604ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _save_results(self, document_path: str, processed_content: List[Dict]) -> Dict:\n",
    "        \n",
    "        output_dir = Path(\"processed_documents\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        doc_name = Path(document_path).stem\n",
    "        base_path = output_dir / doc_name\n",
    "        \n",
    "        # Сохраняем контент\n",
    "        content_path = f\"{base_path}_content.json\"\n",
    "        with open(content_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed_content, f, ensure_ascii=False, indent=2)\n",
    "        print(content_path})\n",
    "            \n",
    "        metadata = {\n",
    "            'document_path': document_path,\n",
    "            'num_chunks': len(processed_content),\n",
    "            'output_files': {\n",
    "                'content': str(content_path)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Сохраняем метаданные\n",
    "        metadata_path = f\"{base_path}_metadata.json\"\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Метаданные сохранены в: {metadata_path}\")\n",
    "            \n",
    "        return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5c06947-6f41-41d2-b840-02658c9537bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def process_document(self, document_path: str) -> ProcessingResult:\n",
    "        doc_result = self._convert_document(document_path)\n",
    "        \n",
    "        chunks = self._get_chunks(doc_result.document)\n",
    "        \n",
    "        processed_content = self._process_content(chunks)\n",
    "        \n",
    "        metadata = self._save_results(document_path, processed_content)\n",
    "        \n",
    "        return ProcessingResult(\n",
    "            chunks=processed_content,\n",
    "            metadata=metadata,\n",
    "            raw_doc=doc_result.document,\n",
    "            chunked_doc=chunks\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f90b5e0-129d-4e05-9a3e-0facfbe3ff79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 18:30:29,973 - INFO - Начало обработки документа: /Users/ivan/Downloads/digital_production_5.pdf\n",
      "2024-12-07 18:30:30,605 - INFO - Going to convert document batch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9c38b945bd4b90ad454be39cbf144e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 18:30:45,525 - INFO - Processing document digital_production_5.pdf\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ProcessingResult:\n",
    "    \"\"\"Результаты обработки документа\"\"\"\n",
    "    chunks: List[Dict]\n",
    "    metadata: Dict\n",
    "    raw_doc: Any\n",
    "    processing_time: float\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, \n",
    "                 chunk_size: int = 300,           # Уменьшенный размер чанка\n",
    "                 chunk_overlap: int = 30,         # Уменьшенное перекрытие\n",
    "                 max_workers: int = 4,            # Количество потоков\n",
    "                 page_batch_size: int = 10):      # Размер пакета страниц\n",
    "        \n",
    "        self.converter = DocumentConverter()\n",
    "        self.chunker = HierarchicalChunker(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            split_on_headings=True\n",
    "        )\n",
    "        self.max_workers = max_workers\n",
    "        self.page_batch_size = page_batch_size\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def _process_page_batch(self, doc, start_page: int, end_page: int) -> List[Dict]:\n",
    "        \"\"\"Обработка пакета страниц\"\"\"\n",
    "        try:\n",
    "            # Получаем элементы только для указанных страниц\n",
    "            elements = [elem for elem in doc.body.get_elements() \n",
    "                       if hasattr(elem, 'page_number') and \n",
    "                       start_page <= elem.page_number <= end_page]\n",
    "            \n",
    "            processed_elements = []\n",
    "            for element in elements:\n",
    "                elem_data = {\n",
    "                    'type': 'text' if hasattr(element, 'text') else 'image',\n",
    "                    'page_number': element.page_number,\n",
    "                    'content': None,\n",
    "                    'metadata': {}\n",
    "                }\n",
    "                \n",
    "                # Обработка текста\n",
    "                if hasattr(element, 'text'):\n",
    "                    elem_data['content'] = element.text\n",
    "                    if hasattr(element, 'meta'):\n",
    "                        elem_data['metadata']['headings'] = element.meta.headings if hasattr(element.meta, 'headings') else []\n",
    "                \n",
    "                # Обработка изображения\n",
    "                elif hasattr(element, 'image_data'):\n",
    "                    elem_data['content'] = 'image_data_present'\n",
    "                    elem_data['metadata']['image_info'] = {\n",
    "                        'format': getattr(element, 'format', 'unknown'),\n",
    "                        'size': getattr(element, 'size', 'unknown')\n",
    "                    }\n",
    "                \n",
    "                processed_elements.append(elem_data)\n",
    "            \n",
    "            return processed_elements\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка при обработке страниц {start_page}-{end_page}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def process_document(self, \n",
    "                        document_path: str, \n",
    "                        start_page: Optional[int] = None, \n",
    "                        end_page: Optional[int] = None) -> ProcessingResult:\n",
    "        \"\"\"Обработка документа с возможностью указания диапазона страниц\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        logger.info(f\"Начало обработки документа: {document_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Конвертация документа\n",
    "            doc_result = self.converter.convert(document_path)\n",
    "            doc = doc_result.document\n",
    "            \n",
    "            # Определение диапазона страниц\n",
    "            total_pages = doc.num_pages if hasattr(doc, 'num_pages') else 0\n",
    "            start_page = start_page or 1\n",
    "            end_page = min(end_page or total_pages, total_pages)\n",
    "            \n",
    "            logger.info(f\"Всего страниц: {total_pages}, обрабатываем: {start_page}-{end_page}\")\n",
    "            \n",
    "            # Разбиваем на пакеты страниц\n",
    "            page_ranges = [\n",
    "                (i, min(i + self.page_batch_size - 1, end_page))\n",
    "                for i in range(start_page, end_page + 1, self.page_batch_size)\n",
    "            ]\n",
    "            \n",
    "            # Параллельная обработка пакетов\n",
    "            processed_chunks = []\n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(self._process_page_batch, doc, start, end)\n",
    "                    for start, end in page_ranges\n",
    "                ]\n",
    "                \n",
    "                # Собираем результаты с прогресс-баром\n",
    "                for future in tqdm(futures, desc=\"Обработка пакетов страниц\"):\n",
    "                    processed_chunks.extend(future.result())\n",
    "            \n",
    "            # Сохранение результатов\n",
    "            metadata = self._save_results(document_path, processed_chunks, start_page, end_page)\n",
    "            \n",
    "            total_time = time.time() - self.start_time\n",
    "            logger.info(f\"Обработка завершена за {total_time:.2f} секунд\")\n",
    "            \n",
    "            return ProcessingResult(\n",
    "                chunks=processed_chunks,\n",
    "                metadata=metadata,\n",
    "                raw_doc=doc,\n",
    "                processing_time=total_time\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка при обработке документа: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def _save_results(self, \n",
    "                     document_path: str, \n",
    "                     processed_content: List[Dict],\n",
    "                     start_page: int,\n",
    "                     end_page: int) -> Dict:\n",
    "        \"\"\"Сохранение результатов\"\"\"\n",
    "        output_dir = Path(\"processed_documents\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        doc_name = Path(document_path).stem\n",
    "        base_path = output_dir / f\"{doc_name}_pages_{start_page}-{end_page}\"\n",
    "        \n",
    "        # Сохранение контента\n",
    "        content_path = f\"{base_path}_content.json\"\n",
    "        with open(content_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed_content, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        metadata = {\n",
    "            'document_path': document_path,\n",
    "            'pages_processed': {\n",
    "                'start': start_page,\n",
    "                'end': end_page\n",
    "            },\n",
    "            'num_chunks': len(processed_content),\n",
    "            'processing_time': time.time() - self.start_time,\n",
    "            'output_files': {\n",
    "                'content': str(content_path)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Сохранение метаданных\n",
    "        metadata_path = f\"{base_path}_metadata.json\"\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        return metadata\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    document_path = \"/Users/ivan/Downloads/digital_production_5.pdf\"\n",
    "    \n",
    "    # Создаем процессор с оптимизированными параметрами\n",
    "    processor = DocumentProcessor(\n",
    "        chunk_size=300,          # Небольшой размер чанка\n",
    "        chunk_overlap=30,        # Минимальное перекрытие\n",
    "        max_workers=4,           # Количество потоков\n",
    "        page_batch_size=5        # Размер пакета страниц\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Обработка всего документа\n",
    "        results = processor.process_document(\n",
    "            document_path,\n",
    "            start_page=1,    # Начальная страница\n",
    "            end_page=79      # Последняя страница\n",
    "        )\n",
    "        \n",
    "        print(\"\\n=== Результаты обработки ===\")\n",
    "        print(f\"Обработано чанков: {len(results.chunks)}\")\n",
    "        print(f\"Время обработки: {results.processing_time:.2f} секунд\")\n",
    "        \n",
    "        # Анализ результатов\n",
    "        text_chunks = [c for c in results.chunks if c['type'] == 'text']\n",
    "        image_chunks = [c for c in results.chunks if c['type'] == 'image']\n",
    "        \n",
    "        print(f\"\\nНайдено текстовых блоков: {len(text_chunks)}\")\n",
    "        print(f\"Найдено изображений: {len(image_chunks)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка при обработке: {str(e)}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5276f8-bd27-463e-b475-c1433f6356ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
