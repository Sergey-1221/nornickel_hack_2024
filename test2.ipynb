{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T16:52:49.469250Z",
     "start_time": "2024-12-07T16:52:46.349393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode\n",
    "from docling_core.transforms.chunker import HierarchicalChunker\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "pipeline_options = PdfPipelineOptions(do_table_structure=True)\n",
    "pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE  # use more accurate TableFormer model\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")"
   ],
   "id": "41e7fcdcf2ab7ce4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from PIL import Image\n",
    "import torch\n",
    "from colpali_engine import ColPali\n",
    "from transformers.models.paligemma.configuration_paligemma import PaliGemmaConfig\n",
    "import gradio as gr\n",
    "\n",
    "def setup_models():\n",
    "    \n",
    "    try:\n",
    "        pali_config = PaliGemmaConfig(\n",
    "            vocab_size=32000,\n",
    "            hidden_size=4096,\n",
    "            intermediate_size=11008,\n",
    "            num_hidden_layers=32,\n",
    "            num_attention_heads=32,\n",
    "            max_position_embeddings=8192,\n",
    "            rms_norm_eps=1e-6,\n",
    "            use_cache=True,\n",
    "            pad_token_id=0,\n",
    "            bos_token_id=1,\n",
    "            eos_token_id=2,\n",
    "            tie_word_embeddings=False,\n",
    "            use_memory_efficient_attention=True,\n",
    "            hidden_act=\"silu\"\n",
    "        )\n",
    "        \n",
    "        # Инициализация ColPali\n",
    "        colpali = ColPali(config=pali_config)\n",
    "        \n",
    "        # Загружаем Qwen2-VL напрямую из transformers\n",
    "        from transformers import Qwen2VLForCausalLM\n",
    "        \n",
    "        qwen_model = Qwen2VLForCausalLM.from_pretrained(\n",
    "            \"Qwen/Qwen2-VL-7B\",\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        qwen_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"Qwen/Qwen2-VL-7B\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        return colpali, qwen_model, qwen_tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке моделей: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_image(image, colpali, qwen_model, qwen_tokenizer):\n",
    "    try:\n",
    "        # ColPali обработка\n",
    "        colpali_result = colpali.process_image(image)\n",
    "        \n",
    "        # Qwen2-VL обработка\n",
    "        qwen_inputs = qwen_tokenizer(\n",
    "            text=\"Describe this image in detail:\",\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(qwen_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            qwen_outputs = qwen_model.generate(\n",
    "                **qwen_inputs,\n",
    "                max_new_tokens=100,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        \n",
    "        qwen_result = qwen_tokenizer.decode(qwen_outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return f\"\"\"\n",
    "{colpali_result}\n",
    "\n",
    "{qwen_result}\n",
    "\"\"\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return (e)\n",
    "\n",
    "def main():\n",
    "    login_to_hf()\n",
    "    \n",
    "    colpali, qwen_model, qwen_tokenizer = setup_models()\n",
    "    \n",
    "    demo = gr.Interface(\n",
    "        fn=lambda img: process_image(img, colpali, qwen_model, qwen_tokenizer),\n",
    "        inputs=gr.Image(type=\"pil\"),\n",
    "        outputs=gr.Textbox(label=\"Результат\"),\n",
    "        title=\"ColPali + Qwen2-VL Demo\",\n",
    "        description=\"Загрузите изображение для анализа обеими моделями\"\n",
    "    )\n",
    "    \n",
    "    demo.launch(share=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "28617d4ff9235952"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
